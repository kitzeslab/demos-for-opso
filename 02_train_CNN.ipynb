{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7571ad0e-9e0b-404d-8910-fa0033c98c50",
   "metadata": {},
   "source": [
    "# Train a CNN to detect bird vocalizations\n",
    "\n",
    "This notebook demonstrates how to train a CNN deep learning model with OpenSoundscape. We will train the CNN to recognize bird vocalizations in spectrogram representations of audio data.\n",
    "\n",
    "The notebook has two sections:\n",
    "1. [Prepare CNN training data from Raven-annotated audio](#prep)\n",
    "2. [Train a CNN](#train)\n",
    "\n",
    "The subsequent notebook evaluates the performance of the CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e235d576-62b7-417c-88c9-05aead69fd75",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Opensoundscape imports\n",
    "from opensoundscape import BoxedAnnotations, CNN\n",
    "\n",
    "# general purpose packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import re # for regex matching of annotation and audio files\n",
    "import random \n",
    "from glob import glob\n",
    "import sklearn\n",
    "\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "plt.rcParams['figure.figsize']=[10,3] #set default graphic size\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9673aa9e-196d-4dd7-ae6a-c51eececf1a4",
   "metadata": {},
   "source": [
    "<a id=\"prep\"></a>\n",
    "\n",
    "## Step 1: Prepare CNN training data from Raven-annotated audio\n",
    "If you have listened to some of your field recordings and annotated them for the presence of your sounds of interest, it's easy to use them as training data to train a classifier using OpenSoundscape. This notebook shows the data processing steps used to turn annotations of audio into the data format used for model training in OpenSoundscape. In this example we are using a set of recordings that were annotated using the software Raven Pro 1.6.4 (Bioacoustics Research Program 2022):\n",
    "\n",
    "<i>An annotated set of audio recordings of Eastern North American birds containing frequency, time, and species information. </i><br>\n",
    "Lauren M. Chronister,  Tessa A. Rhinehart,  Aidan Place,  Justin Kitzes <br>\n",
    "https://doi.org/10.1002/ecy.3329 \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c6e0bd67-2461-4f7f-b620-217a1e37033e",
   "metadata": {},
   "source": [
    "## Download instructions\n",
    "Download the datasets to your current working directory and unzip them. You can do so by running the cell below OR\n",
    "- downloading and unzipping both `annotation_Files.zip` and `mp3_Files.zip` from the https://datadryad.org/stash/dataset/doi:10.5061/dryad.d2547d81z  \n",
    "- Move the unziped contents of each into the folder `./resources/02/annotated_data/` (a subfolder of the current folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f6436cf-a6cd-496e-bf15-5d589b3aa762",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!wget -O annotation_Files.zip https://datadryad.org/stash/downloads/file_stream/641805\n",
    "!wget -O wav_Files.zip https://datadryad.org/stash/downloads/file_stream/641808\n",
    "!unzip annotation_Files.zip -d ./resources/02/annotated_data/Annotation_Files\n",
    "!unzip wav_Files.zip -d ./resources/02/annotated_data/Recordings"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c9b27e16-2460-41f1-9e9f-352665393c78",
   "metadata": {},
   "source": [
    "### Load Raven annotations and create label dataframes\n",
    "The below shows the data munging process of reading in raven files, and using them to create dataframes we can use for training and tset sets for training our model. We will take the annotation files and turn them into a dataframe with 1-hot labels for each 3 second interval - one hot labels that are 1 if a species is present in the audio and 0 if the species is not present in that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2b271061-7195-4a06-8377-7e4d8142aec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the current directory to where the dataset is downloaded\n",
    "dataset_path = Path(\"./resources/02/annotated_data/\").resolve() \n",
    "\n",
    "# make a list of all of the selection table files\n",
    "selections = glob(f\"{dataset_path}/Annotation_Files/*/*.txt\")\n",
    "\n",
    "# Audio files have the same names as selection files\n",
    "audio_files = [f.replace('Annotation_Files','Recordings').replace('.Table.1.selections.txt','.wav') for f in selections]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3c13e96f-b759-4b12-bf9d-710b0f1b8735",
   "metadata": {},
   "source": [
    "selections Loading raven annotations \n",
    "The BoxedAnnotations class stores frequency-time annotations in a table. It can parse and load Raven formatted selection tables with the `from_raven_files()` method. We pass the method a list of raven files and the corresponding list of audio files. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "80c2b97e-2a3f-4d91-8573-b248f7e460dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>audio_file</th>\n",
       "      <th>raven_file</th>\n",
       "      <th>annotation</th>\n",
       "      <th>start_time</th>\n",
       "      <th>end_time</th>\n",
       "      <th>low_f</th>\n",
       "      <th>high_f</th>\n",
       "      <th>View</th>\n",
       "      <th>Channel</th>\n",
       "      <th>Selection</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/Users/LOF19/Dev/demos-for-opso/resources/02/a...</td>\n",
       "      <td>/Users/LOF19/Dev/demos-for-opso/resources/02/a...</td>\n",
       "      <td>BTNW</td>\n",
       "      <td>0.913636</td>\n",
       "      <td>2.202273</td>\n",
       "      <td>4635.1</td>\n",
       "      <td>7439.0</td>\n",
       "      <td>Spectrogram 1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/Users/LOF19/Dev/demos-for-opso/resources/02/a...</td>\n",
       "      <td>/Users/LOF19/Dev/demos-for-opso/resources/02/a...</td>\n",
       "      <td>EATO</td>\n",
       "      <td>2.236363</td>\n",
       "      <td>2.693182</td>\n",
       "      <td>3051.9</td>\n",
       "      <td>4101.0</td>\n",
       "      <td>Spectrogram 1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          audio_file  \\\n",
       "0  /Users/LOF19/Dev/demos-for-opso/resources/02/a...   \n",
       "1  /Users/LOF19/Dev/demos-for-opso/resources/02/a...   \n",
       "\n",
       "                                          raven_file annotation  start_time  \\\n",
       "0  /Users/LOF19/Dev/demos-for-opso/resources/02/a...       BTNW    0.913636   \n",
       "1  /Users/LOF19/Dev/demos-for-opso/resources/02/a...       EATO    2.236363   \n",
       "\n",
       "   end_time   low_f  high_f           View Channel Selection  \n",
       "0  2.202273  4635.1  7439.0  Spectrogram 1       1         1  \n",
       "1  2.693182  3051.9  4101.0  Spectrogram 1       1         2  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_annotations = BoxedAnnotations.from_raven_files(selections,audio_files)\n",
    "all_annotations.df.head(2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "116c2194-53ee-4bff-a4be-31059004c413",
   "metadata": {},
   "source": [
    "This table contains one row per annotation created in Raven pro. \n",
    "We can easily convert this annotation format to a table of 0 (absent) or 1 (present) labels for a series of time-regions in each audio file. Each class will be a separate column. We can specify a list of classes or let the function automatically create one class for each unique annotation in the Raven selection tables. \n",
    "\n",
    "Here, we need to make some choices: first, how many seconds is each audio \"clip\" that we want to generate a label for (clip_duration), and how many seconds of overlap should there be between consecutive clips (clip_overlap)? Here, we'll choose 3 second clips with zero overlap. \n",
    "\n",
    "Second, how much does an annotation need to overlap with a clip for us to consider the annotation to apply to the clip (min_label_overlap)? For example, if an annotation spans 1-3.02 seconds, we might not want to consider it a part of a clip that spans 3-6 seconds, since only 0.02 seconds of that annotation overlap with the clip. Here, we'll choose a min_label_overlap of 0.25 seconds. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "27373fa5-b61f-4a48-bada-3643d80dacee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture cap --no-stdout\n",
    "clip_labels = all_annotations.one_hot_clip_labels(\n",
    "    clip_duration=3,\n",
    "    clip_overlap=0,\n",
    "    min_label_overlap=0.25)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0eeeeb01-cc82-49c6-98fc-f96664f0b993",
   "metadata": {},
   "source": [
    "### choose classes\n",
    "\n",
    "Let's choose 7 classes from the annotated data and train our CNN to recognize vocalizations of these species. The annotations in this dataset use four-letter \"Alpha codes\" for each bird species:\n",
    "\n",
    "- NOCA: Northern Cardinal\n",
    "- EATO: Eastern Towhee\n",
    "- SCTA: Scarlet Tanager\n",
    "- BAWW: Black-and-white Warbler\n",
    "- BCCH: Black-capped Chickadee\n",
    "- AMCR: American Crow\n",
    "- NOFL: Northern Flicker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "696628fa-0c36-4688-869a-93c20ccd73ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "species_of_interest = [\"NOCA\", \"EATO\", \"SCTA\", \"BAWW\", \"BCCH\", \"AMCR\", \"NOFL\"]\n",
    "clip_labels = clip_labels[species_of_interest]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1e5d9ad0-f293-467d-bcf9-df8cec6b3db8",
   "metadata": {},
   "source": [
    "### Split annotated data into training and validation sets\n",
    "\n",
    "Our plan is to train a machine learning model on the files in folders `Recording_1`, `Recording_2` and `Recording_3` and test its performance on recordings in the folder `Recording_4`. Let's separate the labels into two sets called `train` and `test`. We'll use the train set to train the CNN, and the validation set to check how it performs on data that it has not seen during training. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fb43b26b-321e-41a1-b3d9-1572e40f5af4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# select all files from Recording_4 as a test set\n",
    "mask = clip_labels.reset_index()['file'].apply(lambda x: 'Recording_4' in x).values\n",
    "test_set = clip_labels[mask]\n",
    "\n",
    "# all other files will be used as a training set\n",
    "training_set = clip_labels.drop(test_set.index)\n",
    "\n",
    "# Save .csv tables of the training and validation sets for use in training a model\n",
    "# training_set.to_csv(\"./resources/03/training_set.csv\")\n",
    "# test_set.to_csv(\"./resources/03/test_set.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "326e6ee6-2e3c-4c1f-ad69-84bb46a1aa90",
   "metadata": {},
   "source": [
    "Alternatively, load the training and testing set from saved csv files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8d8f68b1-abcf-48ca-8456-98cf9523f0be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training_set = pd.read_csv('./resources/03/training_set.csv',index_col=[0,1,2])\n",
    "# test_set = pd.read_csv('./resources/03/test_set.csv',index_col=[0,1,2])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b274629e-830f-42ec-ab5a-8f7b9d4de2e5",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a id=\"train\"></a>\n",
    "## Train the CNN\n",
    "Now that we have prepared and split our labeled data into training and testing sets, we can train a CNN to recognize the labeled classes. We will further split our training set into `training` and `validation` sets. The idea being that we can 'peek' at the performance on the validation set to choose hyperparameters like how many epochs to train for, but we will not look at the test set until we've finished training our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "545d5063-6964-4e72-b51e-78a9e0dd50b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split our training data into training and validation sets\n",
    "train_df, valid_df = sklearn.model_selection.train_test_split(training_set, test_size=0.1, random_state=0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "94cc19c9-ef94-4297-acac-ff4d11c50435",
   "metadata": {},
   "source": [
    "### Resample data for even class representation\n",
    "\n",
    "Before training, we will balance the number of samples of each class in the training set. This helps the model learn all of the classes, rather than paying too much attention to the classes with the most labeled annotations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b0f0758e-03a5-4c1d-8ab2-6737de01ccbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from opensoundscape.data_selection import resample\n",
    "\n",
    "# upsample (repeat samples) so that all classes have 800 samples\n",
    "balanced_train_df = resample(train_df,n_samples_per_class=800,random_state=0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "597803ea-d8a7-4296-b708-ffa3da85b266",
   "metadata": {},
   "source": [
    "Create the model object. We're using a resnet34 architecture CNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8b369815-3cd7-4500-8349-8207c9d05b9e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create a CNN object designed to recognize 3-second samples\n",
    "# we use the resnet34 architecture, \n",
    "model = CNN('resnet34',classes=species_of_interest,sample_duration=3.0, single_target=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ada11395-9743-4a51-9e17-42055937b8a7",
   "metadata": {},
   "source": [
    "move the model to GPU if available, to accelerate training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9508ccf0-4526-4aa1-94d6-3e2a18c27699",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.device is: mps\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "if torch.backends.mps.is_available():\n",
    "    model.device='mps' #Apple Silicon\n",
    "elif torch.cuda.is_available():\n",
    "    model.device='cuda' #CUDA GPU  \n",
    "print(f'model.device is: {model.device}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "11ec06f5-fe39-40eb-9f9f-578d7fad5510",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### initialize Weights and Biases logging session\n",
    "\n",
    "Note: to use wandb logging, you will need to create an account on the [wandb website](https://wandb.ai/). The first time you use wandb, you'll be asked for an authentication key which can be found in your wandb profile. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ad17ab4c-62bb-46e5-a909-5e4f4b92f4a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "try:\n",
    "    wandb_session = wandb.init(\n",
    "        entity='entity_name', #replace with your entity/group name\n",
    "        project='opensoundscape training demo',\n",
    "        name='Notebook 02: Train CNN',\n",
    "    )\n",
    "except: #if wandb.init fails, don't use wandb logging\n",
    "    print('failed to create wandb session. wandb session will be None')\n",
    "    wandb_session = None"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "006898a3-368d-4e87-85b0-731a67ee5f07",
   "metadata": {},
   "source": [
    "### Train\n",
    "train the model for 30 epochs\n",
    "\n",
    "We use default training parameters, but many aspects of CNN training can be customized (see [this tutorial](http://opensoundscape.org/en/latest/tutorials/cnn_training_advanced.html) for examples)\n",
    "\n",
    "Training can be a slow process, and the speed of training will depend on your computer. If you wish to skip this step, you can simply load the model that this cell would create by uncommenting the subsequent cell and running it instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3c71bfd6-ffb3-4b7a-aba4-1d08e9a8be8c",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Epoch 0\n"
     ]
    }
   ],
   "source": [
    "%%capture --no-stdout --no-display\n",
    "\n",
    "model.train(\n",
    "    balanced_train_df, \n",
    "    valid_df, \n",
    "    epochs = 30, \n",
    "    batch_size= 64, \n",
    "    log_interval=100, #log progress every 100 batches\n",
    "    num_workers = 32, #32 parallelized cpu tasks for preprocessing\n",
    "    wandb_session=wandb_session,\n",
    "    save_interval = 10, #save checkpoint every 10 epochs\n",
    "    save_path = './resources/02/' #location to save checkpoints\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "23069cc7-cbb9-43cf-a18c-23ca4d7bfb23",
   "metadata": {},
   "source": [
    "As training progresses, performance metrics will be plotted to the wandb logging platform and visible on this run's web page. One this cell completes, you have trained the CNN. \n",
    "\n",
    "This [wandb web page](https://wandb.ai/kitzeslab/opensoundscape%20training%20demo/runs/w1xyk7zr/workspace?workspace=user-samlapp) shows the content logged to wandb when this notebook was run by the Kitzes Lab. By default, OpenSoundscape + WandB integration creates several pages with information about the model:\n",
    "- Overview: hyperparameters, run description, and hardware available during the run\n",
    "- Charts: \"Samples\" panel with audio and images of preprocessed samples (useful for checking that your preprocessing performs as expected and your labels are correct)\n",
    "- Charts: graphs of each class's performance metrics over training time\n",
    "- Model: summary of model architecture\n",
    "- Logs: standard output of training script\n",
    "- System: computational performance metrics including memory, CPU use, etc\n",
    "\n",
    "When training several models and comparing performance, the \"Project\" page of WandB provides comparisons of metrics and hyperparameters across training runs.\n",
    "\n",
    "In the next notebook, we will use the CNN to predict on the test set and evaluate its performance. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pip_opso091_py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
