{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7571ad0e-9e0b-404d-8910-fa0033c98c50",
   "metadata": {},
   "source": [
    "# Train a CNN to detect bird vocalizations\n",
    "\n",
    "This notebook demonstrates how to train a CNN deep learning model with OpenSoundscape. We will train the CNN to recognize bird vocalizations in spectrogram representations of audio data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2444c3a4-d44e-4e38-9079-47b2c409545b",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### package imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e235d576-62b7-417c-88c9-05aead69fd75",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Opensoundscape imports\n",
    "from opensoundscape import BoxedAnnotations, CNN\n",
    "\n",
    "# general purpose packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import re # for regex matching of annotation and audio files\n",
    "import random \n",
    "from glob import glob\n",
    "import sklearn\n",
    "\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "plt.rcParams['figure.figsize']=[10,3] #set default graphic size\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9673aa9e-196d-4dd7-ae6a-c51eececf1a4",
   "metadata": {},
   "source": [
    "## Step 1: Prepare CNN training data from Raven-annotated audio\n",
    "\n",
    "If you have listened to some of your field recordings and annotated them for the presence of your sounds of interest, it's easy to use them as training data to train a classifier using OpenSoundscape. This notebook shows the data processing steps used to turn annotations of audio into the data format used for model training in OpenSoundscape. In this example we are using a set of recordings that were annotated using the software Raven Pro 1.6.4 (Bioacoustics Research Program 2022):\n",
    "\n",
    "<i>An annotated set of audio recordings of Eastern North American birds containing frequency, time, and species information. </i><br>\n",
    "Lauren M. Chronister,  Tessa A. Rhinehart,  Aidan Place,  Justin Kitzes <br>\n",
    "https://doi.org/10.1002/ecy.3329 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e0bd67-2461-4f7f-b620-217a1e37033e",
   "metadata": {},
   "source": [
    "## Download instructions\n",
    "Download the datasets to your current working directory and unzip them. You can do so by downloading both `annotation_Files.zip` and `wav_Files.zip` from the url below or by executing the cell below. \n",
    "\n",
    "https://datadryad.org/stash/dataset/doi:10.5061/dryad.d2547d81z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af45cee6-0311-4321-af2d-baa5a0a9888e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!wget -O annotation_Files.zip https://datadryad.org/stash/downloads/file_stream/641805\n",
    "!wget -O wav_Files.zip https://datadryad.org/stash/downloads/file_stream/641808\n",
    "!unzip annotation_Files.zip\n",
    "!unzip wav_Files.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9b27e16-2460-41f1-9e9f-352665393c78",
   "metadata": {},
   "source": [
    "### Load Raven annotations and create label dataframes\n",
    "The below shows the data munging process of reading in raven files, and using them to create dataframes we can use for training and tset sets for training our model. We will take the annotation files and turn them into a dataframe with 1-hot labels for each 3 second interval - one hot labels that are 1 if a species is present in the audio and 0 if the species is not present in that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2b271061-7195-4a06-8377-7e4d8142aec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the current directory to where the dataset is downloaded\n",
    "dataset_path = Path(\"./ecy3329-sup-0001-datas1/\").resolve() \n",
    "\n",
    "# make a list of all of the selection table files\n",
    "selections = glob(f\"{dataset_path}/Annotation_Files/*/*.txt\")\n",
    "\n",
    "# Audio files have the same names as selection files\n",
    "audio_files = [f.replace('Annotation_Files','Recordings').replace('.Table.1.selections.txt','.mp3') for f in selections]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c13e96f-b759-4b12-bf9d-710b0f1b8735",
   "metadata": {},
   "source": [
    "#### Loading raven annotations \n",
    "The BoxedAnnotations class stores frequency-time annotations in a table. It can parse and load Raven formatted selection tables with the `from_raven_files()` method. We pass the method a list of raven files and the corresponding list of audio files. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "80c2b97e-2a3f-4d91-8573-b248f7e460dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>audio_file</th>\n",
       "      <th>raven_file</th>\n",
       "      <th>annotation</th>\n",
       "      <th>start_time</th>\n",
       "      <th>end_time</th>\n",
       "      <th>low_f</th>\n",
       "      <th>high_f</th>\n",
       "      <th>View</th>\n",
       "      <th>Selection</th>\n",
       "      <th>Channel</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/Users/SML161/demos-for-opso/ecy3329-sup-0001-...</td>\n",
       "      <td>/Users/SML161/demos-for-opso/ecy3329-sup-0001-...</td>\n",
       "      <td>BTNW</td>\n",
       "      <td>0.913636</td>\n",
       "      <td>2.202273</td>\n",
       "      <td>4635.1</td>\n",
       "      <td>7439.0</td>\n",
       "      <td>Spectrogram 1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/Users/SML161/demos-for-opso/ecy3329-sup-0001-...</td>\n",
       "      <td>/Users/SML161/demos-for-opso/ecy3329-sup-0001-...</td>\n",
       "      <td>EATO</td>\n",
       "      <td>2.236363</td>\n",
       "      <td>2.693182</td>\n",
       "      <td>3051.9</td>\n",
       "      <td>4101.0</td>\n",
       "      <td>Spectrogram 1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          audio_file  \\\n",
       "0  /Users/SML161/demos-for-opso/ecy3329-sup-0001-...   \n",
       "1  /Users/SML161/demos-for-opso/ecy3329-sup-0001-...   \n",
       "\n",
       "                                          raven_file annotation  start_time  \\\n",
       "0  /Users/SML161/demos-for-opso/ecy3329-sup-0001-...       BTNW    0.913636   \n",
       "1  /Users/SML161/demos-for-opso/ecy3329-sup-0001-...       EATO    2.236363   \n",
       "\n",
       "   end_time   low_f  high_f           View Selection Channel  \n",
       "0  2.202273  4635.1  7439.0  Spectrogram 1         1       1  \n",
       "1  2.693182  3051.9  4101.0  Spectrogram 1         2       1  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_annotations = BoxedAnnotations.from_raven_files(selections,audio_files)\n",
    "all_annotations.df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "116c2194-53ee-4bff-a4be-31059004c413",
   "metadata": {},
   "source": [
    "This table contains one row per annotation created in Raven pro. \n",
    "We can easily convert this annotation format to a table of 0 (absent) or 1 (present) labels for a series of time-regions in each audio file. Each class will be a separate column. We can specify a list of classes or let the function automatically create one class for each unique annotation in the Raven selection tables. \n",
    "\n",
    "Here, we need to make some choices: first, how many seconds is each audio \"clip\" that we want to generate a label for (clip_duration), and how many seconds of overlap should there be between consecutive clips (clip_overlap)? Here, we'll choose 3 second clips with zero overlap. \n",
    "\n",
    "Second, how much does an annotation need to overlap with a clip for us to consider the annotation to apply to the clip (min_label_overlap)? For example, if an annotation spans 1-3.02 seconds, we might not want to consider it a part of a clip that spans 3-6 seconds, since only 0.02 seconds of that annotation overlap with the clip. Here, we'll choose a min_label_overlap of 0.25 seconds. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "27373fa5-b61f-4a48-bada-3643d80dacee",
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_labels = all_annotations.one_hot_clip_labels(\n",
    "    clip_duration=3,\n",
    "    clip_overlap=0,\n",
    "    min_label_overlap=0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e5d9ad0-f293-467d-bcf9-df8cec6b3db8",
   "metadata": {},
   "source": [
    "### split annotated data into training and validation sets\n",
    "\n",
    "Our plan is to train a machine learning model on the files in folders `Recording_1`, `Recording_2` and `Recording_3` and test its performance on recordings in the folder `Recording_4`. Let's separate the labels into two sets called `train` and `validation`. We'll use the train set to train the CNN, and the validation set to check how it performs on data that it has not seen during training. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fb43b26b-321e-41a1-b3d9-1572e40f5af4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# select all files from Recording_4 as a test set\n",
    "mask = clip_labels.reset_index()['file'].apply(lambda x: 'Recording_4' in x).values\n",
    "test_set = clip_labels[mask]\n",
    "\n",
    "# all other files will be used as a training set\n",
    "training_set = clip_labels.drop(test_set.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "530c329c-bc45-4d56-84b0-df6e216e54e3",
   "metadata": {},
   "source": [
    "Save .csv tables of the training and validation sets for use in training a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "45a891b8-80eb-458d-b2df-8f2709aefd96",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set.to_csv(\"./resources/03/training_set.csv\")\n",
    "test_set.to_csv(\"./resources/03/test_set.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b274629e-830f-42ec-ab5a-8f7b9d4de2e5",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Train a CNN\n",
    "Now that we have prepared and split our labeled data into training and testing sets, we can train a CNN to recognize the labeled classes. Let's choose 7 classes from the annotated data and train our CNN to recognize vocalizations of these species:\n",
    "\n",
    "- NOCA: Northern Cardinal\n",
    "- EATO: Eastern Towhee\n",
    "- SCTA: Scarlet Tanager\n",
    "- BAWW: Black-and-white Warbler\n",
    "- BCCH: Black-capped Chickadee\n",
    "- AMCR: American Crow\n",
    "- NOFL: Northern Flicker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bb6bbde1-fd76-4247-b7a4-83947a7b753d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter just to our species of interest\n",
    "species_of_interest = [\"NOCA\", \"EATO\", \"SCTA\", \"BAWW\", \"BCCH\", \"AMCR\", \"NOFL\"]\n",
    "training_set = training_set[species_of_interest]\n",
    "test_set = test_set[species_of_interest]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "545d5063-6964-4e72-b51e-78a9e0dd50b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split our training data into training and validation sets\n",
    "train_df, valid_df = sklearn.model_selection.train_test_split(training_set, test_size=0.1, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94cc19c9-ef94-4297-acac-ff4d11c50435",
   "metadata": {},
   "source": [
    "### Resample data for even class representation\n",
    "\n",
    "Here, we balance the number of samples of each class in the training set. This helps the model learn all of the classes, rather than paying too much attention to the classes with the most labeled annotations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b0f0758e-03a5-4c1d-8ab2-6737de01ccbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this upsamples the less common classes and improves performance\n",
    "# in classes that would otherwise be rare in the training set\n",
    "from opensoundscape.data_selection import resample\n",
    "balanced_train_df = resample(train_df,n_samples_per_class=800,random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "597803ea-d8a7-4296-b708-ffa3da85b266",
   "metadata": {},
   "source": [
    "Create the model object. We're using a resnet34 architecture CNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bbeff554-3cf6-46a8-b0b1-c4cd6dccf441",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create a CNN object designed to recognize 3-second samples\n",
    "# we use the resnet34 architecture, \n",
    "model = CNN('resnet34',classes=species_of_interest,sample_duration=3.0, single_target=False)\n",
    "\n",
    "# if your computer has a GPU, uncomment the relevant line to set model.device\n",
    "# model.device='mps' #uncomment for Apple Silicon GPU use\n",
    "# model.device='cuda' #uncomment for GPUs using cuda"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11ec06f5-fe39-40eb-9f9f-578d7fad5510",
   "metadata": {},
   "source": [
    "#### initialize Weights and Biases logging session\n",
    "\n",
    "Note: to use wandb logging, you will need to create an account on the [wandb website](https://wandb.ai/). The first time you use wandb, you'll be asked for an authentication key which can be found in your wandb profile. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad17ab4c-62bb-46e5-a909-5e4f4b92f4a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "try:\n",
    "    wandb_session = wanb.init(\n",
    "        entity='kitzeslab',\n",
    "        project='opensoundscape training demo',\n",
    "        name='Notebook 03: Train CNN',\n",
    "    )\n",
    "except: #if wandb.init fails, don't use wandb logging\n",
    "    wandb_session = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "006898a3-368d-4e87-85b0-731a67ee5f07",
   "metadata": {},
   "source": [
    "## Training\n",
    "train the model for 30 epochs\n",
    "\n",
    "We use default training parameters, but many aspects of CNN training can be customized (see [this tutorial](http://opensoundscape.org/en/latest/tutorials/cnn_training_advanced.html) for examples)\n",
    "\n",
    "Training can be a slow process, and the speed of training will depend on your computer. If you wish to skip this step, you can simply load the model that this cell would create by uncommenting the subsequent cell and running it instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c71bfd6-ffb3-4b7a-aba4-1d08e9a8be8c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture --no-stdout --no-display\n",
    "\n",
    "model.train(balanced_train_df, valid_df, epochs = 30, batch_size= 64, log_interval=100, num_workers = 32, wandb_session=wandb_session)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23069cc7-cbb9-43cf-a18c-23ca4d7bfb23",
   "metadata": {},
   "source": [
    "As training progresses, performance metrics will be plotted to the wandb logging platform and visible on this run's web page. One this cell completes, you have trained the CNN. In the next notebook, we will use the CNN to predict on the test set and evaluate its performance. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "opso_dev",
   "language": "python",
   "name": "opso_dev"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
