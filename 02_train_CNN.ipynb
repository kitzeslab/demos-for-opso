{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7571ad0e-9e0b-404d-8910-fa0033c98c50",
   "metadata": {},
   "source": [
    "# Train a CNN to detect bird vocalizations\n",
    "\n",
    "This notebook demonstrates how to train a CNN deep learning model with OpenSoundscape. We will train the CNN to recognize bird vocalizations in spectrogram representations of audio data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2444c3a4-d44e-4e38-9079-47b2c409545b",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### package imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e235d576-62b7-417c-88c9-05aead69fd75",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Opensoundscape imports\n",
    "from opensoundscape import BoxedAnnotations, CNN\n",
    "\n",
    "# general purpose packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import re # for regex matching of annotation and audio files\n",
    "import random \n",
    "from glob import glob\n",
    "import sklearn\n",
    "\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "plt.rcParams['figure.figsize']=[10,3] #set default graphic size\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9673aa9e-196d-4dd7-ae6a-c51eececf1a4",
   "metadata": {},
   "source": [
    "## Step 1: Prepare CNN training data from Raven-annotated audio\n",
    "\n",
    "If you have listened to some of your field recordings and annotated them for the presence of your sounds of interest, it's easy to use them as training data to train a classifier using OpenSoundscape. This notebook shows the data processing steps used to turn annotations of audio into the data format used for model training in OpenSoundscape. In this example we are using a set of recordings that were annotated using the software Raven Pro 1.6.4 (Bioacoustics Research Program 2022):\n",
    "\n",
    "<i>An annotated set of audio recordings of Eastern North American birds containing frequency, time, and species information. </i><br>\n",
    "Lauren M. Chronister,  Tessa A. Rhinehart,  Aidan Place,  Justin Kitzes <br>\n",
    "https://doi.org/10.1002/ecy.3329 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e0bd67-2461-4f7f-b620-217a1e37033e",
   "metadata": {},
   "source": [
    "## Download instructions\n",
    "Download the datasets to your current working directory and unzip them. You can do so by downloading both `annotation_Files.zip` and `wav_Files.zip` from the url below or by executing the cell below. \n",
    "\n",
    "https://datadryad.org/stash/dataset/doi:10.5061/dryad.d2547d81z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af45cee6-0311-4321-af2d-baa5a0a9888e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!wget -O annotation_Files.zip https://datadryad.org/stash/downloads/file_stream/641805\n",
    "!wget -O wav_Files.zip https://datadryad.org/stash/downloads/file_stream/641808\n",
    "\n",
    "#TODO: move files into correct subfolders or use as is\n",
    "!unzip annotation_Files.zip\n",
    "!unzip wav_Files.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9b27e16-2460-41f1-9e9f-352665393c78",
   "metadata": {},
   "source": [
    "### Load Raven annotations and create label dataframes\n",
    "The below shows the data munging process of reading in raven files, and using them to create dataframes we can use for training and tset sets for training our model. We will take the annotation files and turn them into a dataframe with 1-hot labels for each 3 second interval - one hot labels that are 1 if a species is present in the audio and 0 if the species is not present in that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2b271061-7195-4a06-8377-7e4d8142aec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the current directory to where the dataset is downloaded\n",
    "dataset_path = Path(\"./ecy3329-sup-0001-datas1/\").resolve() \n",
    "\n",
    "# make a list of all of the selection table files\n",
    "selections = glob(f\"{dataset_path}/Annotation_Files/*/*.txt\")\n",
    "\n",
    "# Audio files have the same names as selection files\n",
    "audio_files = [f.replace('Annotation_Files','Recordings').replace('.Table.1.selections.txt','.mp3') for f in selections]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c13e96f-b759-4b12-bf9d-710b0f1b8735",
   "metadata": {},
   "source": [
    "selections Loading raven annotations \n",
    "The BoxedAnnotations class stores frequency-time annotations in a table. It can parse and load Raven formatted selection tables with the `from_raven_files()` method. We pass the method a list of raven files and the corresponding list of audio files. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "80c2b97e-2a3f-4d91-8573-b248f7e460dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>audio_file</th>\n",
       "      <th>raven_file</th>\n",
       "      <th>annotation</th>\n",
       "      <th>start_time</th>\n",
       "      <th>end_time</th>\n",
       "      <th>low_f</th>\n",
       "      <th>high_f</th>\n",
       "      <th>View</th>\n",
       "      <th>Selection</th>\n",
       "      <th>Channel</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/Users/SML161/demos-for-opso/ecy3329-sup-0001-...</td>\n",
       "      <td>/Users/SML161/demos-for-opso/ecy3329-sup-0001-...</td>\n",
       "      <td>BTNW</td>\n",
       "      <td>0.913636</td>\n",
       "      <td>2.202273</td>\n",
       "      <td>4635.1</td>\n",
       "      <td>7439.0</td>\n",
       "      <td>Spectrogram 1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/Users/SML161/demos-for-opso/ecy3329-sup-0001-...</td>\n",
       "      <td>/Users/SML161/demos-for-opso/ecy3329-sup-0001-...</td>\n",
       "      <td>EATO</td>\n",
       "      <td>2.236363</td>\n",
       "      <td>2.693182</td>\n",
       "      <td>3051.9</td>\n",
       "      <td>4101.0</td>\n",
       "      <td>Spectrogram 1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          audio_file  \\\n",
       "0  /Users/SML161/demos-for-opso/ecy3329-sup-0001-...   \n",
       "1  /Users/SML161/demos-for-opso/ecy3329-sup-0001-...   \n",
       "\n",
       "                                          raven_file annotation  start_time  \\\n",
       "0  /Users/SML161/demos-for-opso/ecy3329-sup-0001-...       BTNW    0.913636   \n",
       "1  /Users/SML161/demos-for-opso/ecy3329-sup-0001-...       EATO    2.236363   \n",
       "\n",
       "   end_time   low_f  high_f           View Selection Channel  \n",
       "0  2.202273  4635.1  7439.0  Spectrogram 1         1       1  \n",
       "1  2.693182  3051.9  4101.0  Spectrogram 1         2       1  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_annotations = BoxedAnnotations.from_raven_files(selections,audio_files)\n",
    "all_annotations.df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "116c2194-53ee-4bff-a4be-31059004c413",
   "metadata": {},
   "source": [
    "This table contains one row per annotation created in Raven pro. \n",
    "We can easily convert this annotation format to a table of 0 (absent) or 1 (present) labels for a series of time-regions in each audio file. Each class will be a separate column. We can specify a list of classes or let the function automatically create one class for each unique annotation in the Raven selection tables. \n",
    "\n",
    "Here, we need to make some choices: first, how many seconds is each audio \"clip\" that we want to generate a label for (clip_duration), and how many seconds of overlap should there be between consecutive clips (clip_overlap)? Here, we'll choose 3 second clips with zero overlap. \n",
    "\n",
    "Second, how much does an annotation need to overlap with a clip for us to consider the annotation to apply to the clip (min_label_overlap)? For example, if an annotation spans 1-3.02 seconds, we might not want to consider it a part of a clip that spans 3-6 seconds, since only 0.02 seconds of that annotation overlap with the clip. Here, we'll choose a min_label_overlap of 0.25 seconds. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "27373fa5-b61f-4a48-bada-3643d80dacee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture cap --no-stdout\n",
    "clip_labels = all_annotations.one_hot_clip_labels(\n",
    "    clip_duration=3,\n",
    "    clip_overlap=0,\n",
    "    min_label_overlap=0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e5d9ad0-f293-467d-bcf9-df8cec6b3db8",
   "metadata": {},
   "source": [
    "### split annotated data into training and validation sets\n",
    "\n",
    "Our plan is to train a machine learning model on the files in folders `Recording_1`, `Recording_2` and `Recording_3` and test its performance on recordings in the folder `Recording_4`. Let's separate the labels into two sets called `train` and `validation`. We'll use the train set to train the CNN, and the validation set to check how it performs on data that it has not seen during training. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fb43b26b-321e-41a1-b3d9-1572e40f5af4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# select all files from Recording_4 as a test set\n",
    "mask = clip_labels.reset_index()['file'].apply(lambda x: 'Recording_4' in x).values\n",
    "test_set = clip_labels[mask]\n",
    "\n",
    "# all other files will be used as a training set\n",
    "training_set = clip_labels.drop(test_set.index)\n",
    "\n",
    "\n",
    "# Save .csv tables of the training and validation sets for use in training a model\n",
    "# training_set.to_csv(\"./resources/03/training_set.csv\")\n",
    "# test_set.to_csv(\"./resources/03/test_set.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "326e6ee6-2e3c-4c1f-ad69-84bb46a1aa90",
   "metadata": {},
   "source": [
    "Alternatively, load the training and testing set from saved csv files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8d8f68b1-abcf-48ca-8456-98cf9523f0be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training_set = pd.read_csv('./resources/03/training_set.csv',index_col=[0,1,2])\n",
    "# test_set = pd.read_csv('./resources/03/test_set.csv',index_col=[0,1,2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b274629e-830f-42ec-ab5a-8f7b9d4de2e5",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Train a CNN\n",
    "Now that we have prepared and split our labeled data into training and testing sets, we can train a CNN to recognize the labeled classes. Let's choose 7 classes from the annotated data and train our CNN to recognize vocalizations of these species. The annotations in this dataset use four-letter \"Alpha codes\" for each bird species:\n",
    "\n",
    "- NOCA: Northern Cardinal\n",
    "- EATO: Eastern Towhee\n",
    "- SCTA: Scarlet Tanager\n",
    "- BAWW: Black-and-white Warbler\n",
    "- BCCH: Black-capped Chickadee\n",
    "- AMCR: American Crow\n",
    "- NOFL: Northern Flicker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bb6bbde1-fd76-4247-b7a4-83947a7b753d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter just to our species of interest\n",
    "species_of_interest = [\"NOCA\", \"EATO\", \"SCTA\", \"BAWW\", \"BCCH\", \"AMCR\", \"NOFL\"]\n",
    "training_set = training_set[species_of_interest]\n",
    "test_set = test_set[species_of_interest]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "545d5063-6964-4e72-b51e-78a9e0dd50b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split our training data into training and validation sets\n",
    "train_df, valid_df = sklearn.model_selection.train_test_split(training_set, test_size=0.1, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94cc19c9-ef94-4297-acac-ff4d11c50435",
   "metadata": {},
   "source": [
    "### Resample data for even class representation\n",
    "\n",
    "Before training, we balance the number of samples of each class in the training set. This helps the model learn all of the classes, rather than paying too much attention to the classes with the most labeled annotations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b0f0758e-03a5-4c1d-8ab2-6737de01ccbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# upsample (repeat samples) so that all classes have 800 samples\n",
    "from opensoundscape.data_selection import resample\n",
    "balanced_train_df = resample(train_df,n_samples_per_class=800,random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "597803ea-d8a7-4296-b708-ffa3da85b266",
   "metadata": {},
   "source": [
    "Create the model object. We're using a resnet34 architecture CNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8b369815-3cd7-4500-8349-8207c9d05b9e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create a CNN object designed to recognize 3-second samples\n",
    "# we use the resnet34 architecture, \n",
    "model = CNN('resnet34',classes=species_of_interest,sample_duration=3.0, single_target=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a2a5f96-d28a-4bbb-b25c-750cf6d10b46",
   "metadata": {},
   "source": [
    "move the model to the desired device (for GPU training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5cbe9845-f63c-4815-bd95-99b16f530c2f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# if your computer has a GPU, uncomment the relevant line to set model.device\n",
    "# model.device='mps' #uncomment to use Apple Silicon GPU\n",
    "model.device='cuda' #uncomment for GPUs using cuda"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11ec06f5-fe39-40eb-9f9f-578d7fad5510",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### initialize Weights and Biases logging session\n",
    "\n",
    "Note: to use wandb logging, you will need to create an account on the [wandb website](https://wandb.ai/). The first time you use wandb, you'll be asked for an authentication key which can be found in your wandb profile. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ad17ab4c-62bb-46e5-a909-5e4f4b92f4a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msamlapp\u001b[0m (\u001b[33mkitzeslab\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.4 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/jet/home/sammlapp/opso_ms/demos-for-opso/wandb/run-20230627_224405-w1xyk7zr</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/kitzeslab/opensoundscape%20training%20demo/runs/w1xyk7zr' target=\"_blank\">Notebook 03: Train CNN</a></strong> to <a href='https://wandb.ai/kitzeslab/opensoundscape%20training%20demo' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/kitzeslab/opensoundscape%20training%20demo' target=\"_blank\">https://wandb.ai/kitzeslab/opensoundscape%20training%20demo</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/kitzeslab/opensoundscape%20training%20demo/runs/w1xyk7zr' target=\"_blank\">https://wandb.ai/kitzeslab/opensoundscape%20training%20demo/runs/w1xyk7zr</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import wandb\n",
    "try:\n",
    "    wandb_session = wandb.init(\n",
    "        entity='entity_name', #replace with your entity/group name\n",
    "        project='opensoundscape training demo',\n",
    "        name='Notebook 02: Train CNN',\n",
    "    )\n",
    "except: #if wandb.init fails, don't use wandb logging\n",
    "    wandb_session = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "006898a3-368d-4e87-85b0-731a67ee5f07",
   "metadata": {},
   "source": [
    "### Train\n",
    "train the model for 30 epochs\n",
    "\n",
    "We use default training parameters, but many aspects of CNN training can be customized (see [this tutorial](http://opensoundscape.org/en/latest/tutorials/cnn_training_advanced.html) for examples)\n",
    "\n",
    "Training can be a slow process, and the speed of training will depend on your computer. If you wish to skip this step, you can simply load the model that this cell would create by uncommenting the subsequent cell and running it instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3c71bfd6-ffb3-4b7a-aba4-1d08e9a8be8c",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Epoch 0\n",
      "Epoch: 0 [batch 0/88, 0.00%] \n",
      "\tDistLoss: 0.794\n",
      "Metrics:\n",
      "Metrics:\n",
      "\tMAP: 0.488\n",
      "\n",
      "Validation.\n",
      "Metrics:\n",
      "\tMAP: 0.476\n",
      "\n",
      "Training Epoch 1\n",
      "Epoch: 1 [batch 0/88, 0.00%] \n",
      "\tDistLoss: 0.376\n",
      "Metrics:\n",
      "Metrics:\n",
      "\tMAP: 0.752\n",
      "\n",
      "Validation.\n",
      "Metrics:\n",
      "\tMAP: 0.626\n",
      "\n",
      "Training Epoch 2\n",
      "Epoch: 2 [batch 0/88, 0.00%] \n",
      "\tDistLoss: 0.278\n",
      "Metrics:\n",
      "Metrics:\n",
      "\tMAP: 0.852\n",
      "\n",
      "Validation.\n",
      "Metrics:\n",
      "\tMAP: 0.757\n",
      "\n",
      "Training Epoch 3\n",
      "Epoch: 3 [batch 0/88, 0.00%] \n",
      "\tDistLoss: 0.246\n",
      "Metrics:\n",
      "Metrics:\n",
      "\tMAP: 0.902\n",
      "\n",
      "Validation.\n",
      "Metrics:\n",
      "\tMAP: 0.745\n",
      "\n",
      "Training Epoch 4\n",
      "Epoch: 4 [batch 0/88, 0.00%] \n",
      "\tDistLoss: 0.216\n",
      "Metrics:\n",
      "Metrics:\n",
      "\tMAP: 0.931\n",
      "\n",
      "Validation.\n",
      "Metrics:\n",
      "\tMAP: 0.806\n",
      "\n",
      "Training Epoch 5\n",
      "Epoch: 5 [batch 0/88, 0.00%] \n",
      "\tDistLoss: 0.175\n",
      "Metrics:\n",
      "Metrics:\n",
      "\tMAP: 0.950\n",
      "\n",
      "Validation.\n",
      "Metrics:\n",
      "\tMAP: 0.856\n",
      "\n",
      "Training Epoch 6\n",
      "Epoch: 6 [batch 0/88, 0.00%] \n",
      "\tDistLoss: 0.154\n",
      "Metrics:\n",
      "Metrics:\n",
      "\tMAP: 0.957\n",
      "\n",
      "Validation.\n",
      "Metrics:\n",
      "\tMAP: 0.842\n",
      "\n",
      "Training Epoch 7\n",
      "Epoch: 7 [batch 0/88, 0.00%] \n",
      "\tDistLoss: 0.135\n",
      "Metrics:\n",
      "Metrics:\n",
      "\tMAP: 0.966\n",
      "\n",
      "Validation.\n",
      "Metrics:\n",
      "\tMAP: 0.838\n",
      "\n",
      "Training Epoch 8\n",
      "Epoch: 8 [batch 0/88, 0.00%] \n",
      "\tDistLoss: 0.166\n",
      "Metrics:\n",
      "Metrics:\n",
      "\tMAP: 0.972\n",
      "\n",
      "Validation.\n",
      "Metrics:\n",
      "\tMAP: 0.849\n",
      "\n",
      "Training Epoch 9\n",
      "Epoch: 9 [batch 0/88, 0.00%] \n",
      "\tDistLoss: 0.098\n",
      "Metrics:\n",
      "Metrics:\n",
      "\tMAP: 0.978\n",
      "\n",
      "Validation.\n",
      "Metrics:\n",
      "\tMAP: 0.840\n",
      "\n",
      "Training Epoch 10\n",
      "Epoch: 10 [batch 0/88, 0.00%] \n",
      "\tDistLoss: 0.093\n",
      "Metrics:\n",
      "Metrics:\n",
      "\tMAP: 0.984\n",
      "\n",
      "Validation.\n",
      "Metrics:\n",
      "\tMAP: 0.853\n",
      "\n",
      "Training Epoch 11\n",
      "Epoch: 11 [batch 0/88, 0.00%] \n",
      "\tDistLoss: 0.064\n",
      "Metrics:\n",
      "Metrics:\n",
      "\tMAP: 0.986\n",
      "\n",
      "Validation.\n",
      "Metrics:\n",
      "\tMAP: 0.859\n",
      "\n",
      "Training Epoch 12\n",
      "Epoch: 12 [batch 0/88, 0.00%] \n",
      "\tDistLoss: 0.115\n",
      "Metrics:\n",
      "Metrics:\n",
      "\tMAP: 0.989\n",
      "\n",
      "Validation.\n",
      "Metrics:\n",
      "\tMAP: 0.839\n",
      "\n",
      "Training Epoch 13\n",
      "Epoch: 13 [batch 0/88, 0.00%] \n",
      "\tDistLoss: 0.084\n",
      "Metrics:\n",
      "Metrics:\n",
      "\tMAP: 0.989\n",
      "\n",
      "Validation.\n",
      "Metrics:\n",
      "\tMAP: 0.826\n",
      "\n",
      "Training Epoch 14\n",
      "Epoch: 14 [batch 0/88, 0.00%] \n",
      "\tDistLoss: 0.079\n",
      "Metrics:\n",
      "Metrics:\n",
      "\tMAP: 0.990\n",
      "\n",
      "Validation.\n",
      "Metrics:\n",
      "\tMAP: 0.791\n",
      "\n",
      "Training Epoch 15\n",
      "Epoch: 15 [batch 0/88, 0.00%] \n",
      "\tDistLoss: 0.053\n",
      "Metrics:\n",
      "Metrics:\n",
      "\tMAP: 0.992\n",
      "\n",
      "Validation.\n",
      "Metrics:\n",
      "\tMAP: 0.821\n",
      "\n",
      "Training Epoch 16\n",
      "Epoch: 16 [batch 0/88, 0.00%] \n",
      "\tDistLoss: 0.041\n",
      "Metrics:\n",
      "Metrics:\n",
      "\tMAP: 0.993\n",
      "\n",
      "Validation.\n",
      "Metrics:\n",
      "\tMAP: 0.816\n",
      "\n",
      "Training Epoch 17\n",
      "Epoch: 17 [batch 0/88, 0.00%] \n",
      "\tDistLoss: 0.077\n",
      "Metrics:\n",
      "Metrics:\n",
      "\tMAP: 0.993\n",
      "\n",
      "Validation.\n",
      "Metrics:\n",
      "\tMAP: 0.803\n",
      "\n",
      "Training Epoch 18\n",
      "Epoch: 18 [batch 0/88, 0.00%] \n",
      "\tDistLoss: 0.043\n",
      "Metrics:\n",
      "Metrics:\n",
      "\tMAP: 0.994\n",
      "\n",
      "Validation.\n",
      "Metrics:\n",
      "\tMAP: 0.826\n",
      "\n",
      "Training Epoch 19\n",
      "Epoch: 19 [batch 0/88, 0.00%] \n",
      "\tDistLoss: 0.043\n",
      "Metrics:\n",
      "Metrics:\n",
      "\tMAP: 0.994\n",
      "\n",
      "Validation.\n",
      "Metrics:\n",
      "\tMAP: 0.814\n",
      "\n",
      "Training Epoch 20\n",
      "Epoch: 20 [batch 0/88, 0.00%] \n",
      "\tDistLoss: 0.059\n",
      "Metrics:\n",
      "Metrics:\n",
      "\tMAP: 0.995\n",
      "\n",
      "Validation.\n",
      "Metrics:\n",
      "\tMAP: 0.820\n",
      "\n",
      "Training Epoch 21\n",
      "Epoch: 21 [batch 0/88, 0.00%] \n",
      "\tDistLoss: 0.082\n",
      "Metrics:\n",
      "Metrics:\n",
      "\tMAP: 0.996\n",
      "\n",
      "Validation.\n",
      "Metrics:\n",
      "\tMAP: 0.817\n",
      "\n",
      "Training Epoch 22\n",
      "Epoch: 22 [batch 0/88, 0.00%] \n",
      "\tDistLoss: 0.031\n",
      "Metrics:\n",
      "Metrics:\n",
      "\tMAP: 0.996\n",
      "\n",
      "Validation.\n",
      "Metrics:\n",
      "\tMAP: 0.831\n",
      "\n",
      "Training Epoch 23\n",
      "Epoch: 23 [batch 0/88, 0.00%] \n",
      "\tDistLoss: 0.030\n",
      "Metrics:\n",
      "Metrics:\n",
      "\tMAP: 0.996\n",
      "\n",
      "Validation.\n",
      "Metrics:\n",
      "\tMAP: 0.814\n",
      "\n",
      "Training Epoch 24\n",
      "Epoch: 24 [batch 0/88, 0.00%] \n",
      "\tDistLoss: 0.045\n",
      "Metrics:\n",
      "Metrics:\n",
      "\tMAP: 0.997\n",
      "\n",
      "Validation.\n",
      "Metrics:\n",
      "\tMAP: 0.822\n",
      "\n",
      "Training Epoch 25\n",
      "Epoch: 25 [batch 0/88, 0.00%] \n",
      "\tDistLoss: 0.037\n",
      "Metrics:\n",
      "Metrics:\n",
      "\tMAP: 0.997\n",
      "\n",
      "Validation.\n",
      "Metrics:\n",
      "\tMAP: 0.808\n",
      "\n",
      "Training Epoch 26\n",
      "Epoch: 26 [batch 0/88, 0.00%] \n",
      "\tDistLoss: 0.033\n",
      "Metrics:\n",
      "Metrics:\n",
      "\tMAP: 0.997\n",
      "\n",
      "Validation.\n",
      "Metrics:\n",
      "\tMAP: 0.800\n",
      "\n",
      "Training Epoch 27\n",
      "Epoch: 27 [batch 0/88, 0.00%] \n",
      "\tDistLoss: 0.040\n",
      "Metrics:\n",
      "Metrics:\n",
      "\tMAP: 0.997\n",
      "\n",
      "Validation.\n",
      "Metrics:\n",
      "\tMAP: 0.808\n",
      "\n",
      "Training Epoch 28\n",
      "Epoch: 28 [batch 0/88, 0.00%] \n",
      "\tDistLoss: 0.035\n",
      "Metrics:\n",
      "Metrics:\n",
      "\tMAP: 0.997\n",
      "\n",
      "Validation.\n",
      "Metrics:\n",
      "\tMAP: 0.810\n",
      "\n",
      "Training Epoch 29\n",
      "Epoch: 29 [batch 0/88, 0.00%] \n",
      "\tDistLoss: 0.037\n",
      "Metrics:\n",
      "Metrics:\n",
      "\tMAP: 0.997\n",
      "\n",
      "Validation.\n",
      "Metrics:\n",
      "\tMAP: 0.812\n",
      "Unexpected exception formatting exception. Falling back to standard exception\n"
     ]
    }
   ],
   "source": [
    "%%capture --no-stdout --no-display\n",
    "\n",
    "model.train(\n",
    "    balanced_train_df, \n",
    "    valid_df, \n",
    "    epochs = 30, \n",
    "    batch_size= 64, \n",
    "    log_interval=100, #log progress every 100 batches\n",
    "    num_workers = 32, #32 parallelized cpu tasks for preprocessing\n",
    "    wandb_session=wandb_session,\n",
    "    save_interval = 10, #save checkpoint every 10 epochs\n",
    "    save_path = './resources/02/' #location to save checkpoints\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23069cc7-cbb9-43cf-a18c-23ca4d7bfb23",
   "metadata": {},
   "source": [
    "As training progresses, performance metrics will be plotted to the wandb logging platform and visible on this run's web page. One this cell completes, you have trained the CNN. \n",
    "\n",
    "This [wandb web page](https://wandb.ai/kitzeslab/opensoundscape%20training%20demo/runs/w1xyk7zr/workspace?workspace=user-samlapp) shows the content logged to wandb when this notebook was run by the Kitzes Lab. By default, OpenSoundscape + WandB integration creates several pages with information about the model:\n",
    "- Overview: hyperparameters, run description, and hardware available during the run\n",
    "- Charts: \"Samples\" panel with audio and images of preprocessed samples (useful for checking that your preprocessing performs as expected and your labels are correct)\n",
    "- Charts: graphs of each class's performance metrics over training time\n",
    "- Model: summary of model architecture\n",
    "- Logs: standard output of training script\n",
    "- System: computational performance metrics including memory, CPU use, etc\n",
    "\n",
    "When training several models and comparing performance, the \"Project\" page of WandB provides comparisons of metrics and hyperparameters across training runs.\n",
    "\n",
    "In the next notebook, we will use the CNN to predict on the test set and evaluate its performance. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "opso091",
   "language": "python",
   "name": "opso091"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
